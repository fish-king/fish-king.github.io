<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>李宏毅强化学习概念理解</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
        <style>
        .ptext{text-indent: 2em;}
        </style>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../index.html" class="title">主页</a>
				<nav>
					<ul>
						<li><a href="#">previous</a></li>
						<li><a href="#">next</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h2 class="major">李宏毅强化学习概念理解</h2>
							<p class="ptext">所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号（强化信号）函数值最大，由于外部给出的信息很少，强化学习系统必须依靠自身的经历进行自我学习。通过这种学习获取知识，改进行动方案以适应环境。强化学习最关键的三个因素是状态，行为和环境奖励。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic1.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">agent即为我们训练索要拟合的函数，输入为environment所提供的observation，输出为我们所需执行的action，当action被执行后，进而environment发生改变，产生新的observation，同时产生reward对我们action好坏进行反馈。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic2.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">上图图为对星球大战游戏模型的样例，我们输入为观测的游戏画面，输出为移动或设计的概率，游戏分数为环境奖励。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic3.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">τ为某一系列observation、action、reward直到游戏结束的过程，R(τ)为reward和，p(τ|θ)为θ状态下得到这一过程的概率。我们判断θ状态下该模型reward即为所有过程reward×概率的和，即期望，由于这个运算情况太多，我们通过多次实验reward求均值代替（概率论中期望的理解）。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic4.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">接下来类比与深度学习，我们为了在θ时action使之后整体reward最大，对模型进行优化。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic5.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic6.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic7.PNG" alt="" />
                                    </span></div>
                            </div>
                            <p class="ptext">使用gradient ascent求解，纯数学化简，有个log微分小细节。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic8.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">最后优化的方程如上图所示，对于positive的reward，我们会提高采取robust action的概率，反正negative reward会降低概率。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../images/blog1/blog1_pic9.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p class="ptext">对于某些概率低取样没有取到的情况（我们每次action训练过程中并不是选概率最大的action，而是按概率选择，最终选择次数和总次数符号概率分布），我们可以通过加bias的方式，只增加base line以上的action概率，减小其他action概率。</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/browser.min.js"></script>
			<script src="../assets/js/breakpoints.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<script src="../assets/js/main.js"></script>

	</body>
</html>