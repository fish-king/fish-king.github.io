<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>李宏毅强化学习概念理解</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../../assets/css/noscript.css" /></noscript>
        <style>
        .ptext{text-indent: 2em;}
        </style>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../../index.html" class="title">主页</a>
				<nav>
					<ul>
						<li><a href="#">previous</a></li>
						<li><a href="blog2.html" class="active">next</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h2 class="major">李宏毅强化学习概念理解</h2>
							<p class="ptext">所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号（强化信号）函数值最大，由于外部给出的信息很少，强化学习系统必须依靠自身的经历进行自我学习。通过这种学习获取知识，改进行动方案以适应环境。强化学习最关键的三个因素是状态，行为和环境奖励。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic1.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">agent即为我们训练索要拟合的函数，输入为environment所提供的observation，输出为我们所需执行的action，当action被执行后，进而environment发生改变，产生新的observation，同时产生reward对我们action好坏进行反馈。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic2.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">上图为对星球大战游戏模型的样例，我们输入为观测的游戏画面，输出为移动或设计的概率，游戏分数为环境奖励。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic3.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">τ为某一系列observation、action、reward直到游戏结束的过程，R(τ)为reward和，p(τ|θ)为模型参数为θ该模型预测得到这一过程的概率。我们判断θ参数下该模型reward即为所有过程reward×概率的和，即期望，该运算状况极多，我们通过多次实验reward求均值代替（概率论中期望的理解）。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic4.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">接下来类比与深度学习，我们为了通过调整θ使模型优化，使得reward增大。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic5.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic6.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic7.PNG" alt="" />
                                    </span></div>
                            </div>
                            <p></p>
                            <p class="ptext">使用gradient ascent求解，纯数学化简，有个log微分小细节。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic8.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">最后优化的方程如上图所示，对于positive的reward，我们会提高采取robust action的概率，反正negative reward会降低概率。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic9.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">对于某些概率低取样没有取到的情况（我们每次action训练过程中并不是选概率最大的action，而是按概率选择，最终选择次数和总次数符号概率分布），我们可以通过加base line的方式，只增加base line以上的action概率，减小其他action概率。一般我们取一次sample成绩的平均值作为base line</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic10.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">公式不难发现，对于同一个trajectory的每一个action我们都分配同样的权值R(τ),这显然不对，我们并不能保证每个action正确，因此对每个action单独分配权值，将做出此action后后续reward和作为权值。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog1/blog1_pic11.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">每一个action对后续的影响并非持续不变的，应该随action增加而减少，因此我们添加discount γ来进行优化。</p>
						</div>
					</section>

			</div>
            <div id="landlord" style="position: fixed; z-index: 10; left: 82%; bottom: 0">

                <div class="message" style="opacity:0"></div>
                <canvas id="live2d" width="280" height="250" class="live2d"></canvas>
                <div class="hide-button">隐藏</div>
            </div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li>
                        <li><span id="busuanzi_value_page_pv"></span> Hits</li>
					</ul>
				</div>
			</footer>
            

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <!--引入css文件-->
            <link rel="stylesheet" href="../../assets/css/live2d.css" />

            <!--引入js文件-->
            <script type="text/javascript">
                    var message_Path = "../../assets/message.json";
                    var home_Path = 'https://fish-king.github.io/'               //此处可以修改为你的域名，必须带斜杠
            </script>
            <script type="text/javascript" src="../../assets/js/live2d.js"></script>
            <script type="text/javascript" src="../../assets/js/message.js"></script>
            <script type="text/javascript">
                loadlive2d("live2d", "../../assets/model/tia/model.json");
            </script>

	</body>
</html>