<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Critic以及Q-learning方法</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../../assets/css/noscript.css" /></noscript>
        <style>
        .ptext{text-indent: 2em;}
        </style>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../../index.html" class="title">主页</a>
				<nav>
					<ul>
						<li><a href="blog2.html" class="active">previous</a></li>
						<li><a href="#">next</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h2 class="major">Critic以及Q-learning方法</h2>
							<p class="ptext">之前我们模型训练都是actor方法，我们通过输入observation，输出为action，相当于训练一个玩游戏的actor,而critic方法是对于我们当前状态预期得分进行评估，输入为observation，输出为state value。两种方法都是强化学习即常用方法。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic1.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">同样以星战游戏举例，当剩余敌机较多，我们飞机防护罩还在情况，模型评估当前的状况好，state value值很大，当剩余敌机较少，我们飞机防护罩消失，则认为我们情况危急，state value值小。我们训练的V^π(s)不止与环境相关，也与actor相关，如果我们actor足够优秀，同样情况下的预测state value也会高，如果actor训练不够，我们对同样情况下的预测state value也会降低。</p>
                            <h3>常见两种critic训练V^π(s)方法</h3>
                            <ul>
                                <li>Monte-Corlo(MC) based approach</li>
                                <li>Temporal-difference(TD) approach</li>
                            </ul>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic2.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">MC方法我们我们的critic先观察actor随机运行多次的数据，计算每个状态累计reward和作为output label，变为一个回归问题。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic3.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">TD方法也是回归的方法，在训练过程中采用两次相邻state差值刚好等于该次action的reward来对模型进行训练。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic4.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">两种方法的差别，对于MC方法，每次训练累计的reward都可能把不同，variance较大，并且每次训练都要求到游戏结束才能获得数据，而TD训练则可以避免这个问题，当然，TD训练过程中，V^π(s)不一定准确的。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic5.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">另一种Critic方法，Q^π(s,a)与V^π(s)区别在于Q中下一步a是固定的，V中下一步则是按a的概率有不同选择。Q按上图有两种架构，一种state,action作为input，输出reward，第二种输入仅state,输出n种reward，对应各个action。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-6"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic6.PNG" alt="" />
                                    </span></div>
                                    <div class="col-6"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic7.PNG" alt="" />
                                    </span></div>
                            </div>
                            <p></p>
                            <p class="ptext">Q-learning方法是对于π的一组episodes，我们对他拟合出Q^π(s,a) function，通过这个function我们能找到更好的π'带入得到新的episodes，继续优化Q^π(s,a)，循环过程。证明在右侧。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic8.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">训练类似TD训练方法，但两个Q相同的话我们训练过程中Q改变，output值会不断变化，因此，我们固定output的Q function，进行N次Regression后用心的替换旧的，继续训练。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic9.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">在训练中，如果某些情况未sample到，那我们对该情况的value估值为初始值，以后的action也不会采用其他值，因此我们需要模型对environment做出exploration，这样sample样例才多样性。有两种方法，Epsilon Greedy和Boltzmann Exploration，图上有详解。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic10.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">训练中，我们采用一个buffer存储数据，buffer中不断增加新数据，满了则删除旧数据。这样有两点优点，其一数据重复使用，这样类似off-policy提升效率，其二提升数据多样性，使模型拟合更好。</p>
                            <div class="row gtr-uniform">
                                    <div class="col-4"></div>
                                    <div class="col-4"><span class="image fit"><img src="../../images/RLImage/blog3/blog3_pic11.PNG" alt="" />
                                    </span></div>
                                    <div class="col-4"></div>
                            </div>
                            <p></p>
                            <p class="ptext">Q-learning模型总结，伪代码流程。</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li>
                        <li><span id="busuanzi_value_page_pv"></span> Hits</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

	</body>
</html>